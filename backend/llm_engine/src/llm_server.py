import asyncio
import grpc
import openai # ìµœì‹  ë²„ì „(v1.0 ì´ìƒ)ì´ ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
import os
import time
from concurrent import futures
from dotenv import load_dotenv

# --- Health Checking ê´€ë ¨ Import ---
from grpc_health.v1 import health_pb2, health_pb2_grpc
from grpc_health.v1.health import HealthServicer
# --- Health Checking ê´€ë ¨ Import ë ---

# ìƒì„±ëœ pb2 ë° pb2_grpc íŒŒì¼ ì„í¬íŠ¸
from generated.llm_pb2 import ChatRequest, ChatResponse
from generated.llm_pb2_grpc import LLMServiceServicer, add_LLMServiceServicer_to_server

# --- OpenAI V1.x í´ë¼ì´ì–¸íŠ¸ ë° httpx Import ---
from openai import AsyncOpenAI
import httpx # httpx ì„í¬íŠ¸ ì¶”ê°€
# --- ---

# OpenAI ì„¤ì •
load_dotenv()

# âœ… ì„¸ì…˜ë³„ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë° ë§ˆì§€ë§‰ ì‚¬ìš© ì‹œê°„ ê´€ë¦¬
session_histories = {}
session_last_used = {}

# ì„¸ì…˜ ìœ ì§€ ì‹œê°„ (ì´ˆ) - 10ë¶„ (600ì´ˆ)
SESSION_TIMEOUT = 600

# ì„¸ì…˜ ìˆ˜ ì œí•œ - 1000ê°œ
MAX_SESSIONS = 1000

# ì„¸ì…˜ ì €ì¥ ê²½ë¡œ
SAVE_DIR = "saved_sessions"
os.makedirs(SAVE_DIR, exist_ok=True)

# --- OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± (summarize_and_save ìš©) ---
# ì´ í•¨ìˆ˜ì—ì„œë„ í”„ë¡ì‹œ ë¹„í™œì„±í™”ëœ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
def get_openai_client():
    transport = httpx.AsyncHTTPTransport(retries=2)
    proxies_disabled_http_client = httpx.AsyncClient(
        proxies=None, 
        transport=transport,
        timeout=60.0 # ìš”ì•½ì€ ë” ê¸¸ê²Œ ì„¤ì • ê°€ëŠ¥
    )
    return AsyncOpenAI(
        api_key=os.getenv("OPENAI_API_KEY"),
        http_client=proxies_disabled_http_client
    )
# --- ---

def build_prompt(session_id, user_input, max_turns=3):
    history = session_histories.get(session_id, [])
    recent_history = history[-max_turns * 2:]

    prompt = []
    for role, content in recent_history:
        prompt.append({"role": role, "content": content})

    prompt.append({"role": "user", "content": user_input})

    return prompt

def update_session_history(session_id, role, content):
    if session_id not in session_histories:
        session_histories[session_id] = []
    session_histories[session_id].append((role, content))
    session_last_used[session_id] = time.time()

def save_session_to_file(session_id):
    history = session_histories.get(session_id, [])
    if not history:
        return

    save_path = os.path.join(SAVE_DIR, f"{session_id}.txt")
    with open(save_path, "w", encoding="utf-8") as f:
        for role, content in history:
            f.write(f"{role.upper()}: {content}\n\n")

    print(f"ğŸ“¦ [Session {session_id}] ì›ë³¸ ì €ì¥ ì™„ë£Œ: {save_path}")

async def summarize_and_save(session_id):
    history = session_histories.get(session_id, [])
    if not history:
        return

    full_conversation = ""
    for role, content in history:
        full_conversation += f"{role}: {content}\n"

    try:
        # --- ìˆ˜ì •ëœ í´ë¼ì´ì–¸íŠ¸ ìƒì„± ë°©ì‹ ì‚¬ìš© ---
        client = get_openai_client() 
        # --- ---
        response = await client.chat.completions.create( 
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì¤˜."},
                {"role": "user", "content": full_conversation}
            ]
        )
        summary = response.choices[0].message.content.strip() if response.choices else "ìš”ì•½ ì‹¤íŒ¨"

        save_path = os.path.join(SAVE_DIR, f"{session_id}_summary.txt")
        with open(save_path, "w", encoding="utf-8") as f:
            f.write(summary)

        print(f"ğŸ“ [Session {session_id}] ìš”ì•½ ì €ì¥ ì™„ë£Œ: {save_path}")

    except openai.APIError as e:
         print(f"âš ï¸  [Session {session_id}] ìš”ì•½ ì‹¤íŒ¨ (OpenAI API Error): {e}")
    except openai.APIConnectionError as e:
         print(f"âš ï¸  [Session {session_id}] ìš”ì•½ ì‹¤íŒ¨ (OpenAI Connection Error): {e}")
    except Exception as e:
        print(f"âš ï¸  [Session {session_id}] ìš”ì•½ ì‹¤íŒ¨ (Unexpected Error): {e}")

async def clean_old_sessions():
    while True:
        now = time.time()
        expired_sessions = [
            session_id for session_id, last_used in list(session_last_used.items()) 
            if now - last_used > SESSION_TIMEOUT
        ]
        for session_id in expired_sessions:
            if session_id in session_histories:
                save_session_to_file(session_id)
                await summarize_and_save(session_id)
                session_histories.pop(session_id, None)
                session_last_used.pop(session_id, None)
                print(f"ğŸ—‘ï¸  [Session {session_id}] ì˜¤ë˜ë˜ì–´ ì‚­ì œë¨")

        if len(session_histories) > MAX_SESSIONS:
            sorted_sessions = sorted(list(session_last_used.items()), key=lambda x: x[1])
            overflow = len(session_histories) - MAX_SESSIONS
            sessions_to_remove = [sid for sid, _ in sorted_sessions[:overflow]]

            for session_id in sessions_to_remove:
                 if session_id in session_histories:
                    save_session_to_file(session_id)
                    await summarize_and_save(session_id)
                    session_histories.pop(session_id, None)
                    session_last_used.pop(session_id, None)
                    print(f"ğŸ§¹ [Session {session_id}] ì„¸ì…˜ ìˆ˜ ì´ˆê³¼ë¡œ ì‚­ì œë¨")

        await asyncio.sleep(60)

class LLMService(LLMServiceServicer):
    # --- OpenAI V1.x: í´ë˜ìŠ¤ ë ˆë²¨ì—ì„œ httpx í´ë¼ì´ì–¸íŠ¸ë¥¼ ì„¤ì •í•˜ì—¬ ì „ë‹¬ (ìˆ˜ì •ë¨) ---
    def __init__(self):
       # 1. í”„ë¡ì‹œê°€ ë¹„í™œì„±í™”ëœ httpx í´ë¼ì´ì–¸íŠ¸ ìƒì„±
       transport = httpx.AsyncHTTPTransport(retries=2)
       proxies_disabled_http_client = httpx.AsyncClient(
           proxies=None,  # í”„ë¡ì‹œ ë¹„í™œì„±í™”
           transport=transport,
           timeout=30.0  # ì˜ˆì‹œ íƒ€ì„ì•„ì›ƒ
       )
       # 2. ìƒì„±ëœ httpx í´ë¼ì´ì–¸íŠ¸ë¥¼ AsyncOpenAIì˜ http_client ì¸ìë¡œ ì „ë‹¬
       self.client = AsyncOpenAI(
           api_key=os.getenv("OPENAI_API_KEY"),
           http_client=proxies_disabled_http_client
       )
       print("ğŸ¤– OpenAI client initialized (proxies explicitly disabled via http_client).")
    # --- ---

    async def ChatStream(self, request_iterator, context):
        # __init__ì—ì„œ ìƒì„±í•œ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
        client = self.client 

        async for request in request_iterator:
            session_id = request.session_id
            user_text = request.user_text

            print(f"ğŸ“ [Session {session_id}] User: {user_text}")

            prompt = build_prompt(session_id, user_text)

            try:
                response_stream = await client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=prompt,
                    stream=True,
                )
                
                assistant_reply = ""
                chunk_count = 0
                async for chunk in response_stream:
                    chunk_count += 1
                    content = chunk.choices[0].delta.content
                    if content:
                        assistant_reply += content
                        try:
                            await context.write(
                                ChatResponse(
                                    assistant_text=content,
                                    is_final=False,
                                )
                            )
                        except grpc.aio.AioRpcError as e:
                            print(f"âš ï¸ [Session {session_id}] Failed to write to stream: {e}")
                            return

                print(f"âœ… [Session {session_id}] Assistant: (received {chunk_count} chunks) {assistant_reply[:50]}...")

                update_session_history(session_id, "user", user_text)
                update_session_history(session_id, "assistant", assistant_reply)

                # if not context.is_active():
                #      print(f"âš ï¸ [Session {session_id}] Client disconnected before sending final response.")
                #      return

                await context.write(
                    ChatResponse(
                        assistant_text="",
                        is_final=True,
                    )
                )
            # --- OpenAI V1.x ì˜¤ë¥˜ ì²˜ë¦¬ ë°©ì‹ ---
            except openai.APIError as e: 
                 print(f"âŒ [Session {session_id}] OpenAI API returned an API Error: {e}")
                 await context.abort(grpc.StatusCode.INTERNAL, f"OpenAI API Error: {e}")
            except openai.APIConnectionError as e: 
                 print(f"âŒ [Session {session_id}] Failed to connect to OpenAI API: {e}")
                 await context.abort(grpc.StatusCode.UNAVAILABLE, f"OpenAI Connection Error: {e}")
            except Exception as e: 
                 print(f"âŒ [Session {session_id}] Unexpected Error in ChatStream: {e}")
                 await context.abort(grpc.StatusCode.INTERNAL, f"Unexpected server error.")
            # --- ---

async def serve():
    health_servicer = HealthServicer(experimental_non_blocking=True)
    server = grpc.aio.server()
    # LLMService ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œ __init__ í˜¸ì¶œë¨
    add_LLMServiceServicer_to_server(LLMService(), server) 
    health_pb2_grpc.add_HealthServicer_to_server(health_servicer, server)
    listen_addr = '[::]:50051'
    server.add_insecure_port(listen_addr)

    print("Starting server...")
    await server.start()
    print(f"âœ… gRPC server started at {listen_addr}")

    health_servicer.set("", health_pb2.HealthCheckResponse.ServingStatus.SERVING)
    health_servicer.set("llm.LLMService", health_pb2.HealthCheckResponse.ServingStatus.SERVING)
    print("âœ… Health status set to SERVING")

    cleaner_task = asyncio.create_task(clean_old_sessions())

    try:
        await server.wait_for_termination()
    except asyncio.CancelledError:
         print("Server task cancelled.")
    finally:
        print("Stopping server...")
        health_servicer.set_overall_serving_status(health_pb2.HealthCheckResponse.ServingStatus.NOT_SERVING)
        await server.stop(grace=5)
        cleaner_task.cancel()
        try:
            await cleaner_task
        except asyncio.CancelledError:
            pass
        print("Server stopped gracefully.")

if __name__ == "__main__":
    try:
        asyncio.run(serve())
    except KeyboardInterrupt:
        print("KeyboardInterrupt received, stopping.")
    except Exception as e:
        print(f"Unhandled exception: {e}")