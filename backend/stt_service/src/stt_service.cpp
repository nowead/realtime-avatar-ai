// stt_service.cpp (ìˆ˜ì • ì œì•ˆëœ ì „ì²´ ì½”ë“œ)

#include "stt_service.h"
#include <iostream>
#include <string>
#include <vector>
#include <future>
#include <atomic>
#include <random>
#include <sstream>
#include <iomanip>
#include <chrono>
#include <thread> // for sleep_for (optional)

// í•„ìš”í•œ ê²½ìš° ì¶”ê°€ í—¤ë”
#include "llm_engine_client.h" // LLMEngineClient ì‚¬ìš© ìœ„í•´ í•„ìš”
#include "azure_stt_client.h" // AzureSTTClient ì‚¬ìš© ìœ„í•´ í•„ìš”
#include <google/protobuf/empty.pb.h> // Empty íƒ€ì… ì‚¬ìš© ìœ„í•´ í•„ìš”
#include "stt.pb.h" // STTStreamRequest ë“± ì‚¬ìš© ìœ„í•´ í•„ìš”

// grpc ìƒíƒœ ì½”ë“œ ì‚¬ìš© í¸ì˜ì„±
using grpc::Status;
using grpc::StatusCode;

namespace stt {

// ê°„ë‹¨í•œ UUID ìƒì„± í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ)
std::string STTServiceImpl::generate_uuid() {
    std::random_device rd;
    std::mt19937_64 gen(rd());
    std::uniform_int_distribution<uint64_t> dis;

    std::stringstream ss;
    ss << std::hex << std::setfill('0');
    ss << std::setw(16) << dis(gen);
    ss << std::setw(16) << dis(gen);
    return ss.str();
}

// ìƒì„±ì (ë³€ê²½ ì—†ìŒ)
STTServiceImpl::STTServiceImpl(std::shared_ptr<AzureSTTClient> azure_client,
                               std::shared_ptr<LLMEngineClient> llm_client)
  : azure_stt_client_(azure_client), llm_engine_client_(llm_client)
{
    if (!azure_stt_client_) {
        throw std::runtime_error("AzureSTTClient cannot be null in STTServiceImpl.");
    }
    if (!llm_engine_client_) {
        throw std::runtime_error("LLMEngineClient cannot be null in STTServiceImpl.");
    }
}

// Client Streaming RPC êµ¬í˜„ (ë‚´ë¶€ ë¡œì§ ìˆ˜ì •ë¨)
Status STTServiceImpl::RecognizeStream(
    ServerContext* context,
    ServerReader<STTStreamRequest>* reader,
    google::protobuf::Empty* response // ìˆ˜ì •ë¨: íƒ€ì… ëª…ì‹œ (using ëŒ€ì‹ )
) {
    const std::string client_peer = context->peer();
    const std::string session_id = generate_uuid();
    std::cout << "âœ… [" << session_id << "] New client connection from: " << client_peer << std::endl;

    std::string language;
    std::atomic<bool> azure_started{false};
    std::atomic<bool> llm_stream_started{false};
    std::atomic<bool> stream_error_occurred{false};
    std::string error_message;

    std::promise<void> azure_processing_complete_promise;
    auto azure_processing_complete_future = azure_processing_complete_promise.get_future();

    // ---= Graceful cleanup lambda function (ë‚´ë¶€ ìˆ˜ì •ë¨) =---
    auto cleanup_resources = [&](bool stop_azure, bool finish_llm) {
         std::cout << "ğŸ§¹ [" << session_id << "] Cleaning up resources... StopAzure=" << stop_azure << ", FinishLLM=" << finish_llm << std::endl;
         if (finish_llm && llm_engine_client_ && llm_stream_started.load()) {
             std::cout << "   Finishing LLM stream..." << std::endl;
             // ìˆ˜ì •ë¨: Status ê°ì²´ í•˜ë‚˜ë§Œ ë°›ìŒ
             grpc::Status llm_status = llm_engine_client_->FinishStream();
             if (!llm_status.ok()) {
                  std::cerr << "   âš ï¸ LLM stream finish error during cleanup: (" << llm_status.error_code() << ") "
                            << llm_status.error_message() << std::endl;
             } else {
                  // ìˆ˜ì •ë¨: llm_summary ê´€ë ¨ ë¡œê¹… ì œê±°
                  std::cout << "   LLM stream finished successfully during cleanup." << std::endl;
             }
             llm_stream_started.store(false);
         }
         if (stop_azure && azure_stt_client_ && azure_started.load()) {
              std::cout << "   Stopping Azure recognition..." << std::endl;
              // í—¤ë” ìˆ˜ì • í›„ ì´ í•¨ìˆ˜ í˜¸ì¶œì´ ê°€ëŠ¥í•´ì•¼ í•¨
              azure_stt_client_->StopContinuousRecognition();
              azure_started.store(false);
         }
    };

    try {
        // --- 1. ì²« ë©”ì‹œì§€ (Config) ì½ê¸° --- (ë³€ê²½ ì—†ìŒ)
        STTStreamRequest initial_request;
        std::cout << "   [" << session_id << "] Waiting for initial RecognitionConfig..." << std::endl;
        if (!reader->Read(&initial_request)) {
            error_message = "Failed to read initial request from client.";
            std::cerr << "âŒ [" << session_id << "] " << error_message << " Peer: " << client_peer << std::endl;
            return Status(StatusCode::INVALID_ARGUMENT, error_message);
        }
        // ... (config ì½ê¸° ë° ìœ íš¨ì„± ê²€ì‚¬ - ê¸°ì¡´ê³¼ ë™ì¼) ...
        if (initial_request.request_data_case() != STTStreamRequest::kConfig) {
            error_message = "Initial request must be RecognitionConfig.";
            std::cerr << "âŒ [" << session_id << "] " << error_message << std::endl;
            return Status(StatusCode::INVALID_ARGUMENT, error_message);
        }
        language = initial_request.config().language();
        if (language.empty()) {
           error_message = "Language code is missing in RecognitionConfig.";
           std::cerr << "âŒ [" << session_id << "] " << error_message << std::endl;
           return Status(StatusCode::INVALID_ARGUMENT, error_message);
        }
        std::cout << "   [" << session_id << "] Config received: Language=" << language << std::endl;


        // --- 2. LLM Engine ìŠ¤íŠ¸ë¦¼ ì‹œì‘ --- (ë³€ê²½ ì—†ìŒ)
        std::cout << "   [" << session_id << "] Starting stream to LLM Engine..." << std::endl;
        if (!llm_engine_client_->StartStream(session_id)) {
            error_message = "Failed to start stream to LLM Engine.";
            std::cerr << "âŒ [" << session_id << "] " << error_message << std::endl;
            stream_error_occurred.store(true);
            return Status(StatusCode::INTERNAL, error_message);
        }
        llm_stream_started.store(true);
        std::cout << "   [" << session_id << "] LLM stream started successfully." << std::endl;


        // --- 3. Azure STT ì½œë°± ì •ì˜ (ë‚´ë¶€ ìˆ˜ì •ë¨) ---

        // TextChunkCallback: Azure -> LLM í…ìŠ¤íŠ¸ ì „ë‹¬
        auto text_callback =
            [this, session_id, &stream_error_occurred, &error_message, llm_client = llm_engine_client_]
            (const std::string& text, bool is_final) { // is_final ì€ Azure ì½œë°±ì—ì„œ ì˜¤ì§€ë§Œ LLM ì „ì†¡ ì‹œ ì‚¬ìš© ì•ˆí•¨

            if (stream_error_occurred.load()) return;

            // ìˆ˜ì •ë¨: is_final ì¸ì ì œê±°í•˜ê³  í˜¸ì¶œ
            if (!llm_client->SendTextChunk(text)) {
                std::cerr << "âŒ [" << session_id << "] Error sending text chunk to LLM Engine. Marking stream as error." << std::endl;
                if (!stream_error_occurred.load()) {
                     error_message = "Failed to forward text chunk to LLM engine.";
                     stream_error_occurred.store(true);
                }
            }
        };

        // RecognitionCompletionCallback: Azure ì¸ì‹ ì™„ë£Œ/ì˜¤ë¥˜ ì‹œ í˜¸ì¶œë¨ (ë³€ê²½ ì—†ìŒ)
        auto completion_callback =
            [this, session_id, &stream_error_occurred, &error_message, &azure_processing_complete_promise]
            (bool success, const std::string& azure_msg) {
            // ... (ê¸°ì¡´ê³¼ ë™ì¼) ...
             std::cout << "â„¹ï¸ [" << session_id << "] Azure STT processing finished. Success: " << success << std::endl;
             if (!success) {
                 std::cerr << "   Azure STT Error: " << azure_msg << std::endl;
                 if (!stream_error_occurred.load()) {
                     error_message = "Azure STT recognition failed: " + azure_msg;
                     stream_error_occurred.store(true);
                 }
             }
             try {
                 azure_processing_complete_promise.set_value();
             } catch (const std::future_error& e) {
                  std::cerr << "â„¹ï¸ [" << session_id << "] Promise already set in completion_callback: " << e.what() << std::endl;
             }
        };


        // --- 4. Azure STT ì¸ì‹ ì‹œì‘ --- (ë³€ê²½ ì—†ìŒ)
        std::cout << "   [" << session_id << "] Starting Azure continuous recognition..." << std::endl;
        if (!azure_stt_client_->StartContinuousRecognition(language, text_callback, completion_callback)) {
            error_message = "Failed to start Azure continuous recognition.";
            std::cerr << "âŒ [" << session_id << "] " << error_message << std::endl;
            stream_error_occurred.store(true);
            cleanup_resources(false, true); // LLM ìŠ¤íŠ¸ë¦¼ë§Œ ì •ë¦¬ ì‹œë„
            return Status(StatusCode::INTERNAL, error_message);
        }
        azure_started.store(true);
        std::cout << "   [" << session_id << "] Azure recognition started successfully." << std::endl;


        // --- 5. ì˜¤ë””ì˜¤ ì²­í¬ ì½ê¸° ë£¨í”„ --- (PushAudioChunk í˜¸ì¶œ ê°€ëŠ¥í•´ì•¼ í•¨)
        STTStreamRequest audio_request;
        size_t total_bytes_received = 0;
        std::cout << "   [" << session_id << "] Waiting for audio chunks from client..." << std::endl;
        while (!stream_error_occurred.load() && reader->Read(&audio_request)) {
            if (context->IsCancelled()) {
                 std::cout << "   [" << session_id << "] Client cancelled the request." << std::endl;
                 error_message = "Request cancelled by client.";
                 stream_error_occurred.store(true);
                 break;
            }

            if (audio_request.request_data_case() == STTStreamRequest::kAudioChunk) {
                const auto& chunk = audio_request.audio_chunk();
                if (!chunk.empty()) {
                    total_bytes_received += chunk.size();
                    // í—¤ë” ìˆ˜ì • í›„ ì´ í•¨ìˆ˜ í˜¸ì¶œì´ ê°€ëŠ¥í•´ì•¼ í•¨
                    azure_stt_client_->PushAudioChunk(
                        reinterpret_cast<const uint8_t*>(chunk.data()),
                        chunk.size()
                    );
                }
            } // ... (else if ë“± ê¸°ì¡´ê³¼ ë™ì¼) ...
             else if (audio_request.request_data_case() == STTStreamRequest::REQUEST_DATA_NOT_SET) {
                 std::cerr << "âš ï¸ [" << session_id << "] Received request with data not set." << std::endl;
             }
              else {
                  std::cerr << "âš ï¸ [" << session_id << "] Received unexpected non-audio chunk data after config (type: "
                            << audio_request.request_data_case() << "). Ignoring." << std::endl;
              }
        } // end while

        // ... (ë£¨í”„ ì¢…ë£Œ ì›ì¸ í™•ì¸ - ê¸°ì¡´ê³¼ ë™ì¼) ...
        if (stream_error_occurred.load()) {
              std::cerr << "âŒ [" << session_id << "] Error occurred, exiting audio reading loop. Reason: " << error_message << std::endl;
        } else {
              std::cout << "â„¹ï¸ [" << session_id << "] Client finished sending audio. Total bytes received: " << total_bytes_received << "." << std::endl;
        }


        // --- 6. í´ë¼ì´ì–¸íŠ¸ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ í›„ ì²˜ë¦¬ --- (StopContinuousRecognition í˜¸ì¶œ ê°€ëŠ¥í•´ì•¼ í•¨)
        if (azure_started.load()) {
            // í—¤ë” ìˆ˜ì • í›„ ì´ í•¨ìˆ˜ í˜¸ì¶œì´ ê°€ëŠ¥í•´ì•¼ í•¨
            azure_stt_client_->StopContinuousRecognition();
        }


        // --- 7. Azure ì²˜ë¦¬ ì™„ë£Œ ëŒ€ê¸° --- (ë³€ê²½ ì—†ìŒ)
        std::cout << "   [" << session_id << "] Waiting for Azure STT processing to complete..." << std::endl;
        std::future_status wait_status = azure_processing_complete_future.wait_for(std::chrono::seconds(30));
        // ... (íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ - ê¸°ì¡´ê³¼ ë™ì¼) ...
        if (wait_status == std::future_status::timeout) {
             std::cerr << "âŒ [" << session_id << "] Timed out waiting for Azure STT completion (30s)." << std::endl;
             if (!stream_error_occurred.load()) {
                 error_message = "Timeout waiting for Azure STT completion.";
                 stream_error_occurred.store(true);
             }
        } else {
             std::cout << "   [" << session_id << "] Azure STT processing completed or error signal received." << std::endl;
        }


        // --- 8. LLM ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ (ë‚´ë¶€ ìˆ˜ì •ë¨) ---
        Status final_llm_status = Status::OK;
        if (llm_stream_started.load()) {
             std::cout << "   [" << session_id << "] Finishing LLM engine stream..." << std::endl;
             // ìˆ˜ì •ë¨: Status ê°ì²´ í•˜ë‚˜ë§Œ ë°›ìŒ
             grpc::Status llm_status = llm_engine_client_->FinishStream();
             final_llm_status = llm_status;
             llm_stream_started.store(false);

             if (!llm_status.ok()) {
                 std::cerr << "âŒ [" << session_id << "] LLM stream finish error: (" << llm_status.error_code() << ") "
                           << llm_status.error_message() << std::endl;
                  if (!stream_error_occurred.load()) {
                     error_message = "Failed to finish LLM stream: " + llm_status.error_message();
                     stream_error_occurred.store(true);
                  }
             } else {
                  // ìˆ˜ì •ë¨: llm_summary ê´€ë ¨ ë¡œê¹… ì œê±°
                  std::cout << "   [" << session_id << "] LLM stream finished successfully." << std::endl;
                  // í•„ìš”í•˜ë‹¤ë©´ ì„œë²„ê°€ Emptyë¥¼ ë°˜í™˜í–ˆìŒì„ ëª…ì‹œì ìœ¼ë¡œ ë¡œê¹…
             }
        } else {
             std::cout << "   [" << session_id << "] LLM stream was not started or already finished, skipping finish." << std::endl;
        }


        // --- 9. ìµœì¢… ìƒíƒœ ë°˜í™˜ --- (ë³€ê²½ ì—†ìŒ)
        std::cout << "ğŸ [" << session_id << "] Processing complete. Final status check." << std::endl;
        if (stream_error_occurred.load()) {
            if (context->IsCancelled()) {
                std::cerr << "âŒ [" << session_id << "] Returning CANCELLED status." << std::endl;
                 cleanup_resources(true, llm_stream_started.load());
                return Status(StatusCode::CANCELLED, "Request cancelled by client during processing.");
            }
            if (!final_llm_status.ok() && final_llm_status.error_code() != StatusCode::CANCELLED) {
                 std::cerr << "âŒ [" << session_id << "] Returning LLM finish error status: (" << final_llm_status.error_code() << ")" << std::endl;
                 cleanup_resources(true, false);
                 return final_llm_status;
            }
            std::cerr << "âŒ [" << session_id << "] Returning INTERNAL error status: " << error_message << std::endl;
            cleanup_resources(true, llm_stream_started.load());
            return Status(StatusCode::INTERNAL, "An internal error occurred: " + error_message);
        } else {
            std::cout << "âœ… [" << session_id << "] Returning OK status." << std::endl;
            return Status::OK;
        }

    } catch (const std::exception& e) {
        // ... (ì˜ˆì™¸ ì²˜ë¦¬ - ê¸°ì¡´ê³¼ ë™ì¼) ...
        std::cerr << "âŒ [" << session_id << "] Unhandled exception in RecognizeStream: " << e.what() << std::endl;
        stream_error_occurred.store(true);
        error_message = "Unhandled exception: " + std::string(e.what());
        cleanup_resources(azure_started.load(), llm_stream_started.load());
        return Status(StatusCode::UNKNOWN, "An unknown exception occurred in the service handler.");
    } catch (...) {
        // ... (ì•Œ ìˆ˜ ì—†ëŠ” ì˜ˆì™¸ ì²˜ë¦¬ - ê¸°ì¡´ê³¼ ë™ì¼) ...
         std::cerr << "âŒ [" << session_id << "] Unknown non-standard exception in RecognizeStream." << std::endl;
         stream_error_occurred.store(true);
         error_message = "Unknown non-standard exception.";
         cleanup_resources(azure_started.load(), llm_stream_started.load());
         return Status(StatusCode::UNKNOWN, "An unknown exception occurred.");
    }
} // end RecognizeStream

} // namespace stt