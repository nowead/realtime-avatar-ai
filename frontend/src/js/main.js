// frontend/src/js/main.js

import '../css/style.css';
import { initWebSocketConnection, closeWebSocket, sendAudioChunk } from './websocket.js';
import { AudioService, mergeChunks } from './audio.js';
import { SileroVAD } from './sileroVadRunner.js';
import { AvatarService } from './avatar.js';

const startBtn = document.getElementById('startBtn');
const stopBtn = document.getElementById('stopBtn');
const statusEl = document.getElementById('status');
const canvasElement = document.getElementById('renderCanvas');

let vad;

// --- ì˜¤ë””ì˜¤ ë²„í¼ë§ ë° VAD ìƒíƒœ ê´€ë¦¬ë¥¼ ìœ„í•œ ë³€ìˆ˜ ---
let activeSegmentChunks = [];        // í˜„ì¬ ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ë¥¼ êµ¬ì„±í•˜ëŠ” ëª¨ë“  ì²­í¬ (íŒ¨ë”© í¬í•¨)
let preSpeechPaddingChunks = [];     // VADê°€ ìŒì„±ì„ ê°ì§€í•˜ê¸° ì „ì˜ Nê°œ ì²­í¬ë¥¼ ì €ì¥í•˜ëŠ” ë²„í¼
const PAD_BEFORE_SPEECH_CHUNKS = 3;  // ìŒì„± ì‹œì‘ ì „ì— ì¶”ê°€í•  ë¹„ìŒì„± ì²­í¬ ìˆ˜ (ì˜ˆ: 3 chunks * 32ms/chunk = ~96ms)
const MAX_SILENCE_AFTER_SPEECH_CHUNKS = 6; // ìŒì„± ê°ì§€ í›„, ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì¢…ë£Œí•˜ê¸°ê¹Œì§€ í—ˆìš©í•  ìµœëŒ€ ì—°ì† ë¹„ìŒì„± ì²­í¬ ìˆ˜ (ì˜ˆ: 6 * 32ms = ~192ms)
const MIN_TOTAL_CHUNKS_FOR_SEGMENT = 5;  // ì „ì†¡í•  ìµœì†Œ ì „ì²´ ì²­í¬ ìˆ˜ (íŒ¨ë”© í¬í•¨, ë„ˆë¬´ ì§§ì€ ìŒì„±ì€ ë¬´ì‹œ)

let vadCurrentlyDetectingSpeech = false; // VADê°€ í˜„ì¬ ìŒì„±ìœ¼ë¡œ íŒë‹¨ ì¤‘ì¸ì§€ ì—¬ë¶€ (ì„¸ê·¸ë¨¼íŠ¸ ì§„í–‰ ì¤‘)
let silentChunksCountSinceLastSpeech = 0; // ë§ˆì§€ë§‰ ìŒì„± ê°ì§€ í›„ ì—°ì†ëœ ë¹„ìŒì„± ì²­í¬ ìˆ˜
// -------------------------------------------------

// (playInt16Audio í•¨ìˆ˜ëŠ” ì´ì „ ë‹µë³€ê³¼ ë™ì¼í•˜ê²Œ ì¡´ì¬í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤)
function playInt16Audio(int16Array, audioContext) {
  // ... (ì´ì „ ë‹µë³€ì˜ playInt16Audio í•¨ìˆ˜ ë‚´ìš©) ...
  if (!audioContext || audioContext.state === 'closed') {
    console.warn('[Playback] AudioContextê°€ ì—†ê±°ë‚˜ ë‹«í˜€ìˆì–´ ì˜¤ë””ì˜¤ë¥¼ ì¬ìƒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
    return;
  }
  const float32Array = new Float32Array(int16Array.length);
  for (let i = 0; i < int16Array.length; i++) {
    float32Array[i] = int16Array[i] / 32768.0;
  }
  const audioBuffer = audioContext.createBuffer(1, float32Array.length, audioContext.sampleRate);
  audioBuffer.copyToChannel(float32Array, 0);
  const sourceNode = audioContext.createBufferSource();
  sourceNode.buffer = audioBuffer;
  sourceNode.connect(audioContext.destination);
  sourceNode.start();
  console.log(`[Playback] ë³‘í•©ëœ ì˜¤ë””ì˜¤ ì²­í¬ ì¬ìƒ ì‹œì‘ (ê¸¸ì´: ${int16Array.length} ìƒ˜í”Œ)`);
}


AvatarService.initialize(canvasElement);

startBtn.addEventListener('click', async () => {
  startBtn.disabled = true;
  stopBtn.disabled = false;
  statusEl.textContent = 'ğŸ”„ ì—°ê²° ì¤‘...';

  initWebSocketConnection('ws://localhost:8000/ws'); // ì‹¤ì œ URLë¡œ ë³€ê²½

  vad = new SileroVAD('/models/silero_vad.onnx');
  await vad.loadModel();

  // ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”
  activeSegmentChunks = [];
  preSpeechPaddingChunks = [];
  vadCurrentlyDetectingSpeech = false;
  silentChunksCountSinceLastSpeech = 0;

  AudioService.onVADFrame = async (currentChunk) => {
    const isSpeechInCurrentChunk = await vad.detect(currentChunk); // í˜„ì¬ ì²­í¬ì— ëŒ€í•œ VAD ê²°ê³¼
    statusEl.textContent = isSpeechInCurrentChunk ? 'ğŸŸ¢ ë§í•˜ëŠ” ì¤‘...' : 'ğŸ”ˆ ë“£ëŠ” ì¤‘...';

    if (isSpeechInCurrentChunk) {
      if (!vadCurrentlyDetectingSpeech) {
        // --- ìƒíƒœ ì „í™˜: ì¹¨ë¬µ -> ìŒì„± (ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘) ---
        vadCurrentlyDetectingSpeech = true; // ìŒì„± í™œë™ ì‹œì‘ìœ¼ë¡œ í‘œì‹œ
        // preSpeechPaddingChunksì— ì €ì¥ëœ (ìŒì„± ì‹œì‘ ì§ì „ì˜) ì²­í¬ë“¤ê³¼ í˜„ì¬ ìŒì„± ì²­í¬ë¡œ ìƒˆ ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘
        activeSegmentChunks = [...preSpeechPaddingChunks, currentChunk];
        // preSpeechPaddingChunksëŠ” ì´ ì„¸ê·¸ë¨¼íŠ¸ì˜ ì‹œì‘ì— ì‚¬ìš©ë˜ì—ˆìœ¼ë¯€ë¡œ, ë‹¤ìŒ ê°ì§€ë¥¼ ìœ„í•´ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ì‹œ ì±„ì›Œì§€ê±°ë‚˜,
        // ëª…ì‹œì ìœ¼ë¡œ ë¹„ì›Œë„ ë©ë‹ˆë‹¤ (í˜„ì¬ ë¡œì§ì—ì„œëŠ” ë‹¤ìŒ ì¹¨ë¬µ ìƒíƒœì—ì„œ ë‹¤ì‹œ ì±„ì›Œì§).
      } else {
        // --- ìƒíƒœ ìœ ì§€: ìŒì„± ê³„ì† ---
        activeSegmentChunks.push(currentChunk); // í˜„ì¬ ìŒì„± ì²­í¬ë¥¼ ì„¸ê·¸ë¨¼íŠ¸ì— ì¶”ê°€
      }
      silentChunksCountSinceLastSpeech = 0; // ìŒì„±ì´ ê°ì§€ë˜ì—ˆìœ¼ë¯€ë¡œ ì¹¨ë¬µ ì¹´ìš´í„° ë¦¬ì…‹
    } else { // í˜„ì¬ ì²­í¬ê°€ ìŒì„±ì´ ì•„ë‹Œ ê²½ìš°
      if (vadCurrentlyDetectingSpeech) {
        // --- ìƒíƒœ ì „í™˜ ì‹œë„: ìŒì„± -> ì¹¨ë¬µ (ì„¸ê·¸ë¨¼íŠ¸ê°€ ì•„ì§ í™œì„± ìƒíƒœì¼ ìˆ˜ ìˆìŒ) ---
        activeSegmentChunks.push(currentChunk); // ì´ ë¹„ìŒì„± ì²­í¬ë„ ì¼ë‹¨ ì„¸ê·¸ë¨¼íŠ¸ì— ì¶”ê°€ (í›„í–‰ íŒ¨ë”©ì˜ ì¼ë¶€)
        silentChunksCountSinceLastSpeech++;

        if (silentChunksCountSinceLastSpeech >= MAX_SILENCE_AFTER_SPEECH_CHUNKS) {
          // --- ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ ì¢…ë£Œ: ì¶©ë¶„í•œ ì‹œê°„ ë™ì•ˆ ì¹¨ë¬µì´ ì§€ì†ë¨ ---
          vadCurrentlyDetectingSpeech = false; // ìŒì„± í™œë™ ì¢…ë£Œë¡œ í‘œì‹œ

          if (activeSegmentChunks.length >= MIN_TOTAL_CHUNKS_FOR_SEGMENT) {
            const merged = mergeChunks(activeSegmentChunks);
            console.log(`[Main] ğŸ“¤ Segment Ended. Sending ${activeSegmentChunks.length} chunks (total samples: ${merged.length}).`);
            
            // ë³‘í•©ëœ ì˜¤ë””ì˜¤ ì¬ìƒ (í…ŒìŠ¤íŠ¸ìš©)
            if (AudioService.audioContext) {
              playInt16Audio(merged, AudioService.audioContext);
            }
            sendAudioChunk(merged); // ì„œë²„ë¡œ ì „ì†¡
          } else {
            console.log(`[Main] Segment too short after padding, discarding. Chunks: ${activeSegmentChunks.length}`);
          }
          activeSegmentChunks = []; // ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ìœ„í•´ ë²„í¼ ë¹„ìš°ê¸°
          // silentChunksCountSinceLastSpeechëŠ” ë‹¤ìŒ ìŒì„± ì‹œì‘ ì‹œ 0ìœ¼ë¡œ ë¦¬ì…‹ë¨.
          // preSpeechPaddingChunksëŠ” ì•„ë˜ì˜ 'ì§€ì†ì ì¸ ì¹¨ë¬µ' ë¡œì§ì—ì„œ ê´€ë¦¬ë¨.
        }
      } else {
        // --- ìƒíƒœ ìœ ì§€: ì§€ì†ì ì¸ ì¹¨ë¬µ (í™œì„± ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ ì—†ìŒ) ---
        preSpeechPaddingChunks.push(currentChunk); // ìµœê·¼ ë¹„ìŒì„± ì²­í¬ë¥¼ pre-padding ë²„í¼ì— ì €ì¥
        if (preSpeechPaddingChunks.length > PAD_BEFORE_SPEECH_CHUNKS) {
          preSpeechPaddingChunks.shift(); // ë²„í¼ í¬ê¸° ìœ ì§€ (ê°€ì¥ ì˜¤ë˜ëœ ê²ƒ ì œê±°)
        }
      }
    }
  };

  await AudioService.initialize();
  statusEl.textContent = 'ğŸŸ¢ í™œì„±í™”ë¨';
});

stopBtn.addEventListener('click', () => {
  startBtn.disabled = false;
  stopBtn.disabled = true;
  statusEl.textContent = 'â¹ï¸ ì¢…ë£Œë¨';

  AudioService.stop();
  closeWebSocket();
});